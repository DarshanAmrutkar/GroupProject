BFS

#include <iostream>
#include <vector>
#include <queue>
#include <omp.h>

using namespace std;

class Graph {
public:
    int numVertices;
    vector<vector<int>> adjacencyList;

    Graph(int numVertices) {
        this->numVertices = numVertices;
        adjacencyList.resize(numVertices);
    }

    void addEdge(int src, int dest) {
        adjacencyList[src].push_back(dest);
    }
};

void parallelBFS(Graph* graph, int startVertex) {
    vector<int> level(graph->numVertices, -1);
    vector<bool> visited(graph->numVertices, false);

    level[startVertex] = 0;
    visited[startVertex] = true;

    queue<int> vertexQueue;
    vertexQueue.push(startVertex);

    while (!vertexQueue.empty()) {
        int currentVertex = vertexQueue.front();
        vertexQueue.pop();

        #pragma omp parallel for
        for (int i = 0; i < graph->adjacencyList[currentVertex].size(); i++) {
            int neighbor = graph->adjacencyList[currentVertex][i];
            if (!visited[neighbor]) {
                #pragma omp critical
                {
                    if (!visited[neighbor]) {
                        level[neighbor] = level[currentVertex] + 1;
                        visited[neighbor] = true;
                        vertexQueue.push(neighbor);
                    }
                }
            }
        }
    }

    cout << "Parallel BFS Traversal:" << endl;
    for (int i = 0; i < graph->numVertices; i++) {
        cout << "Vertex " << i << ": Level " << level[i] << endl;
    }
}

int main() {
    Graph graph(7);
    graph.addEdge(0, 1);
    graph.addEdge(0, 2);
    graph.addEdge(1, 3);
    graph.addEdge(1, 4);

    parallelBFS(&graph, 0);

    return 0;
}



Parallel Bubble Sort:

#include <iostream>
#include <vector>
#include <omp.h>

using namespace std;

void parallelBubbleSort(vector<int>& arr) {
    int n = arr.size();
    bool sorted = false;

    #pragma omp parallel default(none) shared(arr, n, sorted)
    {
        while (!sorted) {
            #pragma omp barrier
            sorted = true;
            #pragma omp for
            for (int i = 0; i < n - 1; i++) {
                if (arr[i] > arr[i + 1]) {
                    swap(arr[i], arr[i + 1]);
                    sorted = false;
                }
            }
        }
    }
}

int main() {
    vector<int> arr = {4, 2, 1, 6, 8, 3};
    
    cout << "Before Sorting: ";
    for (int num : arr) {
        cout << num << " ";
    }
    cout << endl;

    parallelBubbleSort(arr);

    cout << "After Sorting: ";
    for (int num : arr) {
        cout << num << " ";
    }
    cout << endl;

    return 0;
}
```


Parallel Merge Sort:

#include <iostream>
#include <vector>
#include <omp.h>

using namespace std;

void merge(vector<int>& arr, int left, int mid, int right) {
    int n1 = mid - left + 1;
    int n2 = right - mid;

    vector<int> L(n1), R(n2);

    for (int i = 0; i < n1; i++) {
        L[i] = arr[left + i];
    }

    for (int j = 0; j < n2; j++) {
        R[j] = arr[mid + 1 + j];
    }

    int i = 0, j = 0, k = left;
    while (i < n1 && j < n2) {
        if (L[i] <= R[j]) {
            arr[k] = L[i];
            i++;
        } else {
            arr[k] = R[j];
            j++;
        }
        k++;
    }

    while (i < n1) {
        arr[k] = L[i];
        i++;
        k++;
    }

    while (j < n2) {
        arr[k] = R[j];
        j++;
        k++;
    }
}

void parallelMergeSort(vector<int>& arr, int left, int right) {
    if (left < right) {
        int mid = left + (right - left) / 2;

        #pragma omp parallel default(none) shared(arr, left, mid, right)
        {
            #pragma omp sections
            {
                #pragma omp section
                {
                    parallelMergeSort(arr, left, mid);
                }
                #pragma omp section
                {
                    parallelMergeSort(arr, mid + 1, right);
                }
            }
        }

        merge(arr, left, mid, right);
    }
}

int main() {
    vector<int> arr = {4, 2, 1, 6, 8, 3};

    cout << "Before Sorting: ";
    for (int num : arr) {
        cout << num << " ";
    }
    cout << endl;

    parallelMergeSort(arr, 0, arr.size() - 1);

    cout << "After Sorting: ";
    for (int num : arr) {
        cout << num << " ";
    }
    cout << endl;

    return

 0;
}
```

Please note that in both implementations, OpenMP directives are used to parallelize the sorting algorithms. The `#pragma omp parallel` directive creates a parallel region, and the `#pragma omp for` directive distributes the iterations of the loop across multiple threads.




Certainly! Here's an example implementation of parallel reduction for computing the minimum, maximum, sum, and average operations using OpenMP in C++:

```cpp
#include <iostream>
#include <vector>
#include <limits>
#include <omp.h>

using namespace std;

void parallelReduction(const vector<int>& input, int& minimum, int& maximum, int& sum, double& average) {
    int num_threads = omp_get_max_threads();

    minimum = numeric_limits<int>::max();
    maximum = numeric_limits<int>::min();
    sum = 0;

    #pragma omp parallel default(none) shared(input, num_threads, minimum, maximum, sum)
    {
        int local_minimum = numeric_limits<int>::max();
        int local_maximum = numeric_limits<int>::min();
        int local_sum = 0;

        #pragma omp for
        for (int i = 0; i < input.size(); i++) {
            local_minimum = min(local_minimum, input[i]);
            local_maximum = max(local_maximum, input[i]);
            local_sum += input[i];
        }

        #pragma omp critical
        {
            minimum = min(minimum, local_minimum);
            maximum = max(maximum, local_maximum);
            sum += local_sum;
        }
    }

    average = static_cast<double>(sum) / input.size();
}

int main() {
    vector<int> input = {4, 2, 1, 6, 8, 3};

    int minimum, maximum, sum;
    double average;

    parallelReduction(input, minimum, maximum, sum, average);

    cout << "Minimum: " << minimum << endl;
    cout << "Maximum: " << maximum << endl;
    cout << "Sum: " << sum << endl;
    cout << "Average: " << average << endl;

    return 0;
}
```

In this example, the `parallelReduction` function takes an input vector and performs the minimum, maximum, sum, and average operations using OpenMP parallel reduction. The OpenMP `parallel` directive creates a parallel region, and the `#pragma omp for` directive distributes the iterations of the loop across multiple threads. The `#pragma omp critical` directive ensures that the critical section is executed by only one thread at a time to avoid data races when updating the shared reduction variables.




Certainly! Here are CUDA programs for addition of two numbers and matrix multiplication.

1. CUDA Program for Addition of Two Numbers:

```cpp
#include <iostream>

__global__
void addNumbers(int a, int b, int* result) {
    *result = a + b;
}

int main() {
    int a = 5;
    int b = 3;
    int result;

    int* d_result;
    cudaMalloc((void**)&d_result, sizeof(int));

    addNumbers<<<1, 1>>>(a, b, d_result);

    cudaMemcpy(&result, d_result, sizeof(int), cudaMemcpyDeviceToHost);

    std::cout << "The sum of " << a << " and " << b << " is: " << result << std::endl;

    cudaFree(d_result);

    return 0;
}
```

2. CUDA Program for Matrix Multiplication:

```cpp
#include <iostream>

#define N 3

__global__
void matrixMultiplication(int* a, int* b, int* c) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    int sum = 0;
    for (int k = 0; k < N; k++) {
        sum += a[row * N + k] * b[k * N + col];
    }

    c[row * N + col] = sum;
}

int main() {
    int a[N][N] = { {1, 2, 3}, {4, 5, 6}, {7, 8, 9} };
    int b[N][N] = { {9, 8, 7}, {6, 5, 4}, {3, 2, 1} };
    int c[N][N];

    int* d_a;
    int* d_b;
    int* d_c;
    int size = N * N * sizeof(int);

    cudaMalloc((void**)&d_a, size);
    cudaMalloc((void**)&d_b, size);
    cudaMalloc((void**)&d_c, size);

    cudaMemcpy(d_a, a, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_b, b, size, cudaMemcpyHostToDevice);

    dim3 blockDim(N, N);
    dim3 gridDim(1, 1);

    matrixMultiplication<<<gridDim, blockDim>>>(d_a, d_b, d_c);

    cudaMemcpy(c, d_c, size, cudaMemcpyDeviceToHost);

    std::cout << "Matrix C (Result of Multiplication):" << std::endl;
    for (int i = 0; i < N; i++) {
        for (int j = 0; j < N; j++) {
            std::cout << c[i][j] << " ";
        }
        std::cout << std::endl;
    }

    cudaFree(d_a);
    cudaFree(d_b);
    cudaFree(d_c);

    return 0;
}
```

Make sure to compile and run these programs using the CUDA toolkit and appropriate compiler flags.